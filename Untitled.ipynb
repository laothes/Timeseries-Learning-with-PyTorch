{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6650f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import models.seq2seq\n",
    "import models.seq2seqAttention as ssA\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636362d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    " \n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))  ### 不够长度的 补充pad\n",
    "            print(\" seq[i] =\", seq[i])\n",
    "        input = [num_dic[n] for n in seq[0]]  ##  seq = ['manPP', 'women']\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # output = [num_dic[n] for n in ('S' + 'P' * n_step)]  ## test is ok ?\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]  ### 表示输出结果\n",
    " \n",
    "        input_batch.append(np.eye(n_class)[input])  ## np.eye(n_class)[input]  生成 one-hot词向量  5*29\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target)  # not one-hot\n",
    " \n",
    "    # make tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    " \n",
    " \n",
    "# make test batch 测试数据构建\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    " \n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input = [num_dic[n] for n in input_w]\n",
    "    output = [num_dic[n] for n in ('S' + 'P' * n_step)]\n",
    " \n",
    "    input_batch = np.eye(n_class)[input]\n",
    "    output_batch = np.eye(n_class)[output]\n",
    " \n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae3ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5  ##单词长度，不够的用padding补充\n",
    "hidden_dim = 128\n",
    "emb_dim=25\n",
    "# 创建一个含有26个字母以及S E P的字母列表\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "# 创建一个字母列表的字母和下标为键值对的字典\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "seq_data = [['man', 'man'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "n_class = len(num_dic)\n",
    "batch_size = len(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed830d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ssA.Encoder(input_size=n_class, encoder_hidden_size = hidden_dim, decoder_hidden_size = hidden_dim,\n",
    "                                  embedding_size = emb_dim)\n",
    "attention = ssA.Attention(encoder_hidden_size = hidden_dim, decoder_hidden_size = hidden_dim)\n",
    "decoder = ssA.Decoder(output_size = n_class, decoder_hidden_size = hidden_dim, embedding_size = emb_dim, attention = attention)\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ssA.seq2seq(encoder,decoder,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d51e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([29, 25])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 25])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 25])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([29, 25])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 25])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 128])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([29, 281])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([29])\n",
      "seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(29, 25)\n",
      "    (lstm): LSTM(25, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(29, 25)\n",
      "    (lstm): LSTM(25, 128, batch_first=True)\n",
      "    (linear): Linear(in_features=281, out_features=29, bias=True)\n",
      "    (dropout): Dropout(p=0, inplace=False)\n",
      "  )\n",
      ")\n",
      "Encoder(\n",
      "  (embedding): Embedding(29, 25)\n",
      "  (lstm): LSTM(25, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n",
      "Embedding(29, 25)\n",
      "LSTM(25, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "Dropout(p=0, inplace=False)\n",
      "Decoder(\n",
      "  (attention): Attention(\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (embedding): Embedding(29, 25)\n",
      "  (lstm): LSTM(25, 128, batch_first=True)\n",
      "  (linear): Linear(in_features=281, out_features=29, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ")\n",
      "Attention(\n",
      "  (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      ")\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "Embedding(29, 25)\n",
      "LSTM(25, 128, batch_first=True)\n",
      "Linear(in_features=281, out_features=29, bias=True)\n",
      "Dropout(p=0, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(type(p), p.shape)\n",
    "for n in model.modules():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d02f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#设置优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96c7afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seq[i] = manPP\n",
      " seq[i] = manPP\n",
      " seq[i] = black\n",
      " seq[i] = white\n",
      " seq[i] = kingP\n",
      " seq[i] = queen\n",
      " seq[i] = girlP\n",
      " seq[i] = boyPP\n",
      " seq[i] = upPPP\n",
      " seq[i] = downP\n",
      " seq[i] = highP\n",
      " seq[i] = lowPP\n"
     ]
    }
   ],
   "source": [
    "input_batch, output_batch, target_batch = make_batch()\n",
    "input_batch=input_batch.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec6d245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dc01412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch.shape: torch.Size([6, 5, 29])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 4D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# input_batch : [batch_size, max_len(=n_step, time step), n_class]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_batch.shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m,input_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# output : [max_len+1, batch_size, n_class]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, max_len+1(=6), n_class]\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Desktop\\study\\nn\\models\\seq2seqAttention.py:96\u001b[0m, in \u001b[0;36mseq2seq.forward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     92\u001b[0m trg_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39moutput_size\n\u001b[0;32m     94\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(trg_len, batch_size, trg_vocab_size)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 96\u001b[0m enc_output, s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m dec_input \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_len):\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Desktop\\study\\nn\\models\\seq2seqAttention.py:19\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, enc_x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, enc_x):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# x -> [batch size, sequence len]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(enc_x))  \u001b[38;5;66;03m# [batch size, sequence len, embedding size]\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     output, (ht, ct) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# output -> [batch_size, seq_len, encoder_hidden_size * 2]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# ht -> [n_layers * num_directions, batch_size, encoder_hidden_size]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# ct -> [n_layers * num_directions, batch_size, encoder_hidden_size]\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     forward_ht \u001b[38;5;241m=\u001b[39m ht[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, :, :]  \u001b[38;5;66;03m# [batch_size, encoder_hidden_size]\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\ProgramFile\\Anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1074\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m-> 1074\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1077\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m   1078\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 4D instead"
     ]
    }
   ],
   "source": [
    "for epoch in range(5000):\n",
    "    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "    hidden = torch.zeros(1, batch_size,hidden_dim)  ## 隐层向量初始化\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
    "    # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
    "    # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
    "    print(\"input_batch.shape:\",input_batch.shape)\n",
    "    output = model(input_batch, output_batch)\n",
    "    # output : [max_len+1, batch_size, n_class]\n",
    "    output = output.transpose(0, 1)  # [batch_size, max_len+1(=6), n_class]\n",
    "    loss = 0\n",
    "    for i in range(0, len(target_batch)):\n",
    "        # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "        loss += criterion(output[i], target_batch[i])\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(' now is starting test ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d8bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e01d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3680e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac4cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be10d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813165b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
